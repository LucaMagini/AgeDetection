{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOIfg1kdHfbIwn+jdXc6Eaz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## DATA AUGMENTED\n"],"metadata":{"id":"EAQCmaylz5SY"}},{"cell_type":"code","source":["import tensorflow as tf\n","import keras\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from zipfile import ZipFile\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import regularizers\n","\n","#JUPYTER VERSION\n","#comb_path = \"../Data/Combined_Images\"\n","\n","#GOOGLE COLAB VERSION\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Unzipping the dataset file facial-age.zip\n","\n","#combined_images_path = \"/content/drive/MyDrive/Data/Combined_Images.zip\"\n","combined_images_path = \"/content/drive/MyDrive/Data/Combined_Images_Augmented.zip\"\n","\n","with ZipFile(combined_images_path, 'r') as myzip:\n","    myzip.extractall(\"../content/Combined_Images_Augmented\")\n","    print('Done unzipping Combined_Images_Augmented.zip')   "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9JI4YSKz99E","executionInfo":{"status":"ok","timestamp":1662653937198,"user_tz":-120,"elapsed":67997,"user":{"displayName":"Luca Mag","userId":"06619506036873405402"}},"outputId":"2e11b9a0-42d6-457f-f8e0-9b8d0147e8a0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Done unzipping Combined_Images_Augmented.zip\n"]}]},{"cell_type":"code","source":["comb_path = '../content/Combined_Images_Augmented' \n","batch_size = 128\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","  comb_path,\n","  validation_split=0.2,\n","  subset=\"training\", #If should be return the training set (80%) or the validation set (20%)\n","  seed=41, #Seed should guarantee that train_ds and val_ds doesn't have common images\n","  shuffle=True,\n","  image_size=(50, 50),\n","  batch_size=batch_size,\n","  #color_mode='grayscale'\n","  )\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","  comb_path,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=41, \n","  shuffle=True,\n","  image_size=(50, 50),\n","  batch_size=batch_size,\n","  #color_mode='grayscale'\n","  )\n","\n","test_dataset = val_ds.take(360)\n","val_ds = val_ds.skip(360)\n","\n","print('Batches for training -->', train_ds.cardinality())\n","print('Batches for validating -->', val_ds.cardinality())\n","print('Batches for testing -->', test_dataset.cardinality()) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9fcyXenaSrM","executionInfo":{"status":"ok","timestamp":1662654005592,"user_tz":-120,"elapsed":24761,"user":{"displayName":"Luca Mag","userId":"06619506036873405402"}},"outputId":"e1e050c5-e0b1-4ce4-c6f3-0f04f37c4185"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 237188 files belonging to 8 classes.\n","Using 189751 files for training.\n","Found 237188 files belonging to 8 classes.\n","Using 47437 files for validation.\n","Batches for training --> tf.Tensor(1483, shape=(), dtype=int64)\n","Batches for validating --> tf.Tensor(11, shape=(), dtype=int64)\n","Batches for testing --> tf.Tensor(360, shape=(), dtype=int64)\n"]}]},{"cell_type":"code","source":["data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip(\"horizontal\"),\n","  ####tf.keras.layers.RandomHeight(0.3),\n","  ####tf.keras.layers.RandomWidth(0.3),\n","  tf.keras.layers.RandomRotation(0.35),\n","  tf.keras.layers.RandomZoom(-0.25, 0.25),\n","  tf.keras.layers.RandomContrast(0.2),\n","  #tf.keras.layers.RandomBrightness([-0.8,0.8]),  #Need Tensorflow 2.5.0\n","  tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2)\n","])\n","\n","#resize = tf.keras.layers.Resizing(out_height, out_width)\n","#height = tf.keras.layers.RandomHeight(0.3)\n","#width = tf.keras.layers.RandomWidth(0.3)\n","#zoom = tf.keras.layers.RandomZoom(0.3)\n","#flip = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\") # or \"horizontal\", \"vertical\"\n","#rotate = tf.keras.layers.RandomRotation(0.2)\n","#crop = tf.keras.layers.RandomCrop(out_height, out_width)\n","#translation = tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2)\n","#brightness = tf.keras.layers.RandomBrightness([-0.8,0.8])\n","#contrast = tf.keras.layers.RandomContrast(0.2)"],"metadata":{"id":"nKA9NqZ11RJM","executionInfo":{"status":"aborted","timestamp":1662653173349,"user_tz":-120,"elapsed":20,"user":{"displayName":"Luca Mag","userId":"06619506036873405402"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#RandomHeight and RadomWidth will lead to a None shape on the height dimension, as not all outputs from the layer will be the same height (by design). That is ok for things like the \n","#Conv2D layer, which can accept variable shaped image input (with None shapes on some dimensions).\n","#This will not work for then calling into a Flatten followed by a Dense, because the flattened batches will also be of variable size (because of the variable height), and the \n","#Dense layer needs a fixed shape for the last dimension. You could probably pad output of flatten before the dense, but if you want this architecture, you may just want to \n","#avoid image augmentation layer that lead to a variable output shape.\n","\n","#So instead of using a Flatten layer, you could, for example, use a GlobalMaxPool2D layer, which does not need to know the other dimensions beforehand\n","\n","model4 = tf.keras.Sequential([\n","  tf.keras.layers.Rescaling(1./255, input_shape=(50, 50, 3)),\n","  #data_augmentation,\n","  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'), #kernel_regularizer=regularizers.l2(l=0.01)\n","  tf.keras.layers.MaxPooling2D(),\n","  tf.keras.layers.Dropout(0.4),\n","  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'), #\n","  tf.keras.layers.MaxPooling2D(),\n","  tf.keras.layers.Dropout(0.4),\n","  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'), #\n","  tf.keras.layers.MaxPooling2D(),\n","  tf.keras.layers.Flatten(), #See reference above\n","  #tf.keras.layers.GlobalMaxPool2D(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(8, activation='softmax')\n","])\n","\n","#model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              #loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              #metrics=tf.keras.metrics.Accuracy())\n","\n","#Additionaly, if you do not one-hot encode your data, set sparse_categorical_crossentropy as loss and sparse_categorical_accuracy as metric.\n","model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n","\n","# Defining the early stop to monitor the validation loss to avoid overfitting.\n","early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n","\n","epochs=30\n","history = model4.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  callbacks=[early_stop],\n","  epochs=epochs,\n","  shuffle=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPuhYyAz1hwP","executionInfo":{"status":"ok","timestamp":1662654843410,"user_tz":-120,"elapsed":709320,"user":{"displayName":"Luca Mag","userId":"06619506036873405402"}},"outputId":"420a27d3-7417-4c3b-d052-c51bf0c7af51"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","1483/1483 [==============================] - 53s 28ms/step - loss: 1.8190 - sparse_categorical_accuracy: 0.2908 - val_loss: 1.7045 - val_sparse_categorical_accuracy: 0.3397\n","Epoch 2/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.6833 - sparse_categorical_accuracy: 0.3419 - val_loss: 1.6658 - val_sparse_categorical_accuracy: 0.3500\n","Epoch 3/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.6254 - sparse_categorical_accuracy: 0.3637 - val_loss: 1.5945 - val_sparse_categorical_accuracy: 0.3854\n","Epoch 4/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.5919 - sparse_categorical_accuracy: 0.3745 - val_loss: 1.5712 - val_sparse_categorical_accuracy: 0.3751\n","Epoch 5/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.5674 - sparse_categorical_accuracy: 0.3859 - val_loss: 1.5871 - val_sparse_categorical_accuracy: 0.3751\n","Epoch 6/30\n","1483/1483 [==============================] - 39s 26ms/step - loss: 1.5470 - sparse_categorical_accuracy: 0.3925 - val_loss: 1.5734 - val_sparse_categorical_accuracy: 0.3913\n","Epoch 7/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.5305 - sparse_categorical_accuracy: 0.3987 - val_loss: 1.5579 - val_sparse_categorical_accuracy: 0.3928\n","Epoch 8/30\n","1483/1483 [==============================] - 39s 26ms/step - loss: 1.5157 - sparse_categorical_accuracy: 0.4041 - val_loss: 1.4921 - val_sparse_categorical_accuracy: 0.4090\n","Epoch 9/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.5010 - sparse_categorical_accuracy: 0.4099 - val_loss: 1.5084 - val_sparse_categorical_accuracy: 0.4156\n","Epoch 10/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.4897 - sparse_categorical_accuracy: 0.4138 - val_loss: 1.4868 - val_sparse_categorical_accuracy: 0.4208\n","Epoch 11/30\n","1483/1483 [==============================] - 41s 27ms/step - loss: 1.4775 - sparse_categorical_accuracy: 0.4214 - val_loss: 1.4720 - val_sparse_categorical_accuracy: 0.4208\n","Epoch 12/30\n","1483/1483 [==============================] - 40s 27ms/step - loss: 1.4646 - sparse_categorical_accuracy: 0.4251 - val_loss: 1.4173 - val_sparse_categorical_accuracy: 0.4473\n","Epoch 13/30\n","1483/1483 [==============================] - 41s 28ms/step - loss: 1.4554 - sparse_categorical_accuracy: 0.4301 - val_loss: 1.4546 - val_sparse_categorical_accuracy: 0.4289\n","Epoch 14/30\n","1483/1483 [==============================] - 41s 28ms/step - loss: 1.4453 - sparse_categorical_accuracy: 0.4335 - val_loss: 1.5014 - val_sparse_categorical_accuracy: 0.4245\n","Epoch 15/30\n","1483/1483 [==============================] - 43s 29ms/step - loss: 1.4384 - sparse_categorical_accuracy: 0.4373 - val_loss: 1.4372 - val_sparse_categorical_accuracy: 0.4348\n","Epoch 16/30\n","1483/1483 [==============================] - 44s 30ms/step - loss: 1.4291 - sparse_categorical_accuracy: 0.4408 - val_loss: 1.5214 - val_sparse_categorical_accuracy: 0.4223\n","Epoch 17/30\n","1483/1483 [==============================] - 46s 31ms/step - loss: 1.4213 - sparse_categorical_accuracy: 0.4441 - val_loss: 1.4371 - val_sparse_categorical_accuracy: 0.4348\n","Epoch 17: early stopping\n"]}]},{"cell_type":"code","source":["print(\"MODEL 2 --- With Data Augment\")\n","model4.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSHPoYROlheB","executionInfo":{"status":"ok","timestamp":1662655226644,"user_tz":-120,"elapsed":12974,"user":{"displayName":"Luca Mag","userId":"06619506036873405402"}},"outputId":"5a391502-0eb1-479b-cc43-ebb836e59066"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["MODEL 2 --- With Data Augment\n","360/360 [==============================] - 9s 24ms/step - loss: 1.4063 - sparse_categorical_accuracy: 0.4493\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.4063137769699097, 0.44930556416511536]"]},"metadata":{},"execution_count":4}]}]}